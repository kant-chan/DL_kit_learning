{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36564bit365pyenv5ca0e22325744dcb9c847a4582088af3",
   "display_name": "Python 3.6.5 64-bit ('3.6.5': pyenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Intro\n",
    "\n",
    "An RL agent may include one or more of these components:\n",
    "\n",
    "* Policy: agent's behavior function\n",
    "* Value function: how good is each state or actions\n",
    "* Model: agent's state representation of the environment\n",
    "\n",
    "### 1.1 Policy\n",
    "\n",
    "* A policy is the agent's behavior model\n",
    "* It is a map function from state/observation to action\n",
    "* Stochastic policy: Probabilistic sample $\\pi(a|s) = P[A_t=a|S_t=s]$\n",
    "* Deterministic policy: $a^* = \\underset{a}{\\operatorname{argmax}} \\pi(a|s)$\n",
    "\n",
    "### 1.2 Value function\n",
    "\n",
    "* Value function: expected discounted sum of future rewards under a particular policy $\\pi$\n",
    "* Discount factor weights immediate vs future rewards\n",
    "* Used to quantify goodness/badness of states and actions\n",
    "\n",
    "$$v_\\pi(s) = E_\\pi[G_t|S_t=s] = E_{\\pi}[\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}|S_t=s], \\text{for all } s\\in S $$\n",
    "\n",
    "* Q-function (could be used to select among actions):\n",
    "\n",
    "$$q_\\pi(s, a) = E_\\pi[G_t|S_t=s, A_t=a] = E_{\\pi}[\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}|S_t=s, A_t=a]$$\n",
    "\n",
    "### 1.3 Model\n",
    "\n",
    "A model predicts what the environment will do next. Predict the next state:\n",
    "\n",
    "$$P_{ss'}^{a} = P[S_{t+1}=s'|S_t=s,A_t=a]$$\n",
    "\n",
    "Predict the next reward:\n",
    "\n",
    "$$R_s^a = E[R_{t+1}|S_t=s,A_t=a]$$\n",
    "\n",
    "### 1.4 Markov Decision Processes (MDPs)\n",
    "\n",
    "Definition of MDP:\n",
    "\n",
    "* $P^a$ is dynamics/transition model for each action, $P(S_{t+1}=s'|S_t=s,A_t=a)$\n",
    "* R is a reward function $R(S_t=s,A_t=a) = E[R_t|S_t=s,A_t=a]$\n",
    "* Discount factor: $\\gamma \\in [0, 1]$\n",
    "\n",
    "\n",
    "### 1.5 Types of RL Agents based on what the agent learns\n",
    "\n",
    "* Value-based agent\n",
    "    * explicit: value function\n",
    "    * implicit: policy (can derive a policy from value function)\n",
    "* Policy-based agent\n",
    "    * explicit: policy\n",
    "    * no value function\n",
    "* Actor-Critic agent\n",
    "    * explicit: policy and value function\n",
    "\n",
    "### 1.6 Types of RL Agents on if there is model\n",
    "\n",
    "* Model-based\n",
    "    * explicit: model\n",
    "    * May or may not have policy and/or value function\n",
    "* Model-free\n",
    "    * explicit: value function and/or policy function\n",
    "    * No model\n",
    "\n",
    "<img src=\"../imgs/RL/rl_types.png\" height=\"321\" width=\"342\" />\n",
    "\n",
    "## Q learning\n",
    "\n",
    "### 算法\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q(s, a) & \\leftarrow Q(s, a) + \\alpha [r + \\gamma max_{a'}Q(s', a') - Q(s, a)] \\\\\n",
    "s & \\leftarrow s'\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## Sarsa\n",
    "\n",
    "### 算法\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q(s, a) & \\leftarrow Q(s, a) + \\alpha [r + \\gamma Q(s', a') - Q(s, a)] \\\\\n",
    "s & \\leftarrow s', a \\leftarrow a'\n",
    "\\end{aligned}\n",
    "$$"
   ]
  }
 ]
}