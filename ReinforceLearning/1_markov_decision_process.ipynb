{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Markov Decision Process\n",
    "\n",
    "<img src=\"../imgs/RL/mdp.png\" width=\"441\" height=\"167\" />\n",
    "\n",
    "* Markov Decision Process can model a lot of real-world problem. It formally describes the framework of reinforcement learning\n",
    "\n",
    "* Under MDP, the environment is fully observable.\n",
    "    * Optimal control primarily deals with continuous MDPs\n",
    "    * Partially observable problems can be converted into MDPs\n",
    "\n",
    "\n",
    "### 1.1 Markov Property\n",
    "\n",
    "* The history of states: $h_t = {s_1, s_2, s_3, ..., s_t}$\n",
    "* State $s_t$ is Markovian if and only if:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(s_{t+1}|s_t) &= p(s_{t+1}|h_t) \\\\\n",
    "p(s_{t+1}|s_t,a_t) &= p(s_{t+1}|h_t,a_t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "* The future is independent of the past given the present\n",
    "\n",
    "### 1.2 Markov Process / Markov Chain\n",
    "\n",
    "* State transition matrix P specifies $p(s_{t+1}=s'|s_t=s)$:\n",
    "\n",
    "$$\n",
    "P =\n",
    "\\begin{bmatrix}\n",
    "P(s_1|s_1) & P(s_2|s_1) & \\cdots & P(s_N|s_1) \\\\\n",
    "P(s_1|s_2) & P(s_2|s_2) & \\cdots & P(s_N|s_2) \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "P(s_1|s_N) & P(s_2|s_N) & \\cdots & P(s_N|s_N)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "<img src=\"../imgs/RL/transition.png\" width=\"207\" height=\"166\" />"
   ]
  }
 ]
}